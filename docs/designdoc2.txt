CS130 Project 2 - Design Document
=================================

Please answer all questions in this design document.  Note that the final
feedback section is optional, and you are not required to answer it if you
don't want to.

Unanswered or incompletely answered questions, or answers that don't actually
match the code/repository, will result in deductions.

Answers don't have to be deeply detailed!  We are mainly looking for an
overview or summary description of how your project works, and your team's
experiences working on this project.

Logistics (7 pts)
-----------------

L1.  [2pts] Enumerate all teammates here.

Esmir Mesic, Anya Vinogradsky, Jonathon Corrales

L2.  [2pts] What did each teammate focus on during this project?
Esmir focused on fixing old functionality, writing tests for old functionality 
and stress tests, and JSON

Jon focused on the new sheet features and writing tests

Anya focused on update notifications, debugging, documenting, and
writing tests and stress tests

L3.  [3pts] Approximately how many hours did each teammate spend on the project?
Esmir: 27
Jon: 27
Anya: 29

Spreadsheet Engine Design (16 pts)
----------------------------------

D1.  [4pts] Briefly describe how your workbook-loading code operates.  Does
     it do anything sophisticated to optimize the performance of loading a
     workbook, such as deferring cell-value update calculations, or analyzing
     the graph of cell dependencies?

     The workbook loading does not do any optimizations (coming next week)
     It reads in the dictionaries from the JSON file, throws errors accordingly,
     and runs the workbook commands on each cell (like you would create a cell
     normally)


D2.  [4pts] Sheet-level operations like copying a sheet, deleting a sheet,
     renaming a sheet, or even creating a new sheet, can cause cell values
     to be updated.  How does your workbook identify such cells and ensure
     that they are updated properly?

     Our workbook identifies what cells needs to be updated by traversing our 
     reference graph and identifying cells with references to any sheets that
     have been copied, deleted, renamed, or don't exist yet (in the case of 
     created). Once it identifies such cells, it either manually calls for a 
     topological sort of cells starting at each cell we've identified and
     then again updates all the cell obtained by that sort, or it sets the 
     cell's contents does the same thing after updating it's contents.


D3.  [8pts] When renaming a sheet, cells with formulas that explicitly
     reference the renamed sheet must be updated with the new sheet name.
     Give an overview of how your implementation updates these formulas.
     
     Our implementation updates these formulas by first going through our 
     references graph and updating it such that any reference to the previous
     name of the sheet is updated to use the new sheet name, then traversing 
     this graph once again parsing through the contents of any cell that 
     references this sheet and replacing any occurence of the old sheet name 
     with the new sheet name (checking that those occurences are not sub strings
     of a larger sheet name). It then stores the new contents along with the 
     referencer information to loop through right after to set the cell contents
     of each identified cell to the new contents, which then topologically sorts 
     and updates any relevant cells.
     


Spreadsheet Engine Code Review (24 pts)
---------------------------------------

R1.  [4pts] What part of your spreadsheet code-base did your team select to
     review?  Why did you select this portion of your code to review?

     We reviewed the expression handling/parsing. This part of the code seemed
     to contribute a very large amount to the total runtime of the stress tests
     and had some code that could be simplified/refactored.

R2.  [4pts] Approximately how much code did your team review?  How much time did
     the meeting take?

     We reviewed approximately 300 lines of code (although some of it was repeated
     and needs to be refactored)? The meeting took approx 40 minutes

R3.  [8pts] Include the list of action-items that were generated by the
     code-review here.
     
     Simplify/refactor matching logic with helper function
     Simplify cell method in handler
     Cache parse trees
     Change grammar and parser to use lalr

R4.  [4pts] Have you been able to address the action items identified?  If not,
     what is your plan and timeframe to do so?
     
     We will plan to address these action items in the beginning few days of the
     next project (since they also pertain to a large portion of our slow code)

R5.  [4pts] Reflect on whether the code review helped with knowledge-sharing in
     the team.  Do teammates feel more confident maintaining the code that was
     reviewed, and/or extending the code in the future?
     
     This code review helped with sharing knowledge on the parser which is a bit
     confusing to an extent so now we should all be able to contribute more to
     it in the future, rather than just one person primarily focusing on it. We
     are confident this code will live long into the future!


Performance Analysis (22 pts)
-----------------------------

In this project you must measure and analyze the performance of two central
areas of your spreadsheet engine.  Using pair programming, construct some
performance tests to exercise these aspects of your engine, and use a profiler
to identify where your program is spending the bulk of its time.

A1.  [4pts] Briefly enumerate the performance tests you created to exercise
     your implementation, along with the teammates that collaborated to
     implement each of them.
     
     The first test is a large diamond dependency test, where we have a 5 layer
     diamond structure adding each cell below it in the diamond. Changing the
     root of the diamond's value triggers a LOT of cell updates, so we stress
     test that a lot. (Jon, Esmir)

     We have a second test that runs the same diamond over multiple sheets. (Anya)
     
     We have a smaller diamond test that checks our rename function
     performance in a similar manner, where we have a bunch of cells referencing
     one cell and call rename many times. (Jon, Esmir)
     
     We have 2 tests for checking copy and delete that are similar 
     that run copy/delete many times. (Jon, Esmir)


A2.  [4pts] What profiler did you choose to run your performance tests with?
     Why?  Give an example of how to invoke one of your tests with the profiler.
     
     We tried using pyinstrument but it did not give us insight into the number
     of calls, so we changed to using cprofiler. In our setup we call the 
     profiler constructor, we enable the profiler with self.profiler.enable()
     and when we want to get results we do:
     
          p = Stats(self.profiler)
          p.strip_dirs().dump_stats(filename)
     
     giving a filename to dump the stats into. We can then visualize the dump
     with a library called snakeviz.

A3.  [8pts] What are ~3 of the most significant hot-spots you identified in your
     performance testing?  Did you expect these hot-spots, or were they
     surprising to you?

     Parsing is incredibly slow as expected since we do not cache the parsed tree
     in the cell and instead re-parse the contents every time, we plan to change 
     that for the next project.

     One other improvement we can potentially make to parsing if caching the parse
     trees is not sufficient is to switch from using earley to lalr. We were surprised
     by how long the earley parsing took in general and are worried that just caching
     will not be enough.

     An unexpected hot-spot in terms of number of calls was hashing. We suspect
     this is because we often loop through reference dictionaries and have not
     been careful about storing reused values from these dictionaries instead of
     getting them from the dictionaries every time. We also plan to change this 
     for the next project. 

A4.  [6pts] Reflect on the experience of pair-programming as you constructed
     these tests.  What went well with it?  What would you like to try to do
     better in the future?

     The experience of pair-programming as we constructed these tests went
     fairly smoothly. Having multiple people helped fill in testing gaps that 
     may have gone unnoticed otherwise. Having completed most of the features 
     before doing the review also helped because we had a better grasp of project 2
     as a whole and could more quickly identifying bugs. Occasionally bugs would 
     be a result of different expectations of how certain things were 
     represented in our model, so a more in-depth pre-coding meeting to set 
     expectations after roles have been assigned would help in the future. 


Section F:  CS130 Project 3 Feedback [OPTIONAL]
-----------------------------------------------

These questions are OPTIONAL, and you do not need to answer them.  Your grade
will not be affected by answering or not answering them.  Also, your grade will
not be affected by negative feedback - we want to know what went poorly so that
we can improve future versions of the course.

F1.  What parts of the assignment did you find highly enjoyable?  Conversely,
     what parts of the assignment did you find unenjoyable?


F2.  What parts of the assignment helped you learn more about software
     engineering best-practices, or other useful development skills?
     What parts were not helpful in learning these skills?


F3.  Were there any parts of the assignment that seemed _unnecessarily_ tedious?
     (Some parts of software development are always tedious, of course.)


F4.  Do you have any feedback and/or constructive criticism about how this
     project can be made better in future iterations of CS130?

     It may be useful to have faux "client meetings" with course staff at some 
     point during project timeline as might happen in industry, where some 
     preliminary smoke tests might be run. It can be really difficult to 
     determine if we're considering sufficient edge cases and some feedback 
     would really help fill in gaps in testing
